<!-- omit in toc -->
# CycleGAN: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks
<!-- omit in toc -->
### Janaína Moreira-Kanaley & Taichi Uno

<!-- omit in toc -->
## Table of Content
<!-- TOC -->
- [Introduction](#introduction)
- [Data](#data)
- [CycleGAN](#cyclegan)
- [Evaluation](#evaluation)
- [Results](#results)
- [Conclusions](#conclusions)
- [References](#references)
- [Who did what](#who-did-what)
<!-- TOC -->

## Introduction
Image translation involves converting an input image into an output image, and it finds applications in tasks such as photo enhancement (e.g., improving low-quality images), style transfer (e.g., transforming a photo into a painting), and object transfiguration (e.g., changing a zebra into a horse). This task usually requires aligned pairs of images for training the model[[1](#CNN_Segmentation)]. However, collecting paired annotations can be expensive and thus there is a limited availability of such data.

In our study, we explore the use of Cycle-consistent Generative Adversarial Networks (CycleGAN) as a way for image translation tasks without the need for paired examples[[2](#CycleGAN)]. CycleGAN can learn to transform unpaired images from one domain to another by enforcing cycle consistency, ensuring that the generated images maintain content consistency with the original ones.

Our first task is to replicate and verify the results presented in the CycleGAN paper[[2](#CycleGAN)]. For this task, we will retrain the model using the Cityscapes dataset, which provides paired labels and corresponding photos[[3](#Cityscapes_Dataset)] available from this link[^1]. While the model is capable of handling unpaired images, we will perform quantitative evaluations using the paired labels and photos from this dataset to measure its accuracy in generating realistic scenes or assigning correct labels to segmented sections of cityscapes.

In our second task, we aim to demonstrate the model's ability to work with new unpaired datasets from different domains. We have compiled two datasets containing images of pandas[^2] and brown bears[^3]. The goal of this object transformation task is to convert images of pandas into brown bears (and vice versa), exploiting the similarity between the two animals as they belong to the same family. Since there are no domain-complementary pairs among the images, we cannot use methods like the FCN score[[4](#FCN_score)] based on ground truth image pairs to quantitatively measure the quality and accuracy of the model's outputs.

By performing these two tasks, we aim to validate whether CycleGAN can serve as a versatile image translation model, capable of adapting to different domains without the need for costly paired datasets.

[^1]: https://www.cityscapes-dataset.com/dataset
[^2]: https://images.cv/dataset/panda-image-classification-dataset
[^3]: https://images.cv/dataset/brown-bear-image-classification-dataset

## Data
For the first task, Cityscapes dataset[^1] was used and it consists of urban street "photos" as one domain and a simplified annotated version of the scenes segmented by classes or "labels" (e.g road, building, person) as the other. 

## CycleGAN
This section explains how CycleGAN works in terms of its formulation and implementation. The following information was described in the CycleGAN paper[[2](#CycleGAN)].

CycleGAN implements a model that aims to learn mapping functions between two distinct domains ($X$, $Y$). The mapping functions are represented by $G: X \rightarrow Y$ and $F: Y \rightarrow X$. Adversarial losses are employed for both functions, with $G$ attempting to generate images that resemble those in domain $Y$ and deceive the discriminator $D_y$, while $D_y$ aims to distinguish between real samples and images generated by $G$. To prevent the network from mapping input images to arbitrary permutations of images in the target domain, the model learns mappings that exhibit cycle consistency.

The network architecture comprises three convolutions with a stride of 1/2, nine residual blocks (for 256x256 images), two fractionally-strided convolutions with a stride of 1/2, and one convolutional layer that maps features to the RGB format. For the discriminator networks, the model employs 70 × 70 PatchGANs, which classify whether 70 × 70 overlapping image patches are real or fake. This enables the model to handle images of varying sizes through a fully convolutional approach.

## Evaluation
For evaluation of the reproduction of the result of cityscapes, the Fully Convolutional Network (FCN) score from  [[3](#FCN_score)] will be calculated and compared to results in the paper [[2](#CycleGAN)]. The score measures the pixel-level agreement between the predicted image from CycleGAN and the ground truth image pair. 

For the evaluation of the second task, we intended to carry out a perceptual study if time allows, where users are asked to find the real not-generated image from a list of image pairs. This method gives an indication of the model’s ability to generate believable images for a domain given new datasets. However, it was not possible due to the time constraint as it took us longer than expected to make the model run with our available computational resources and environment.

## Results

## Conclusions

## References
<a id="CNN_Segmentation">[1]</a> J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,”CoRR, vol. abs/1411.4038, 2014. arXiv: 1411.4038. [Online]. Available: http://arxiv.org/abs/1411.4038.

<a id="CycleGAN">[2]</a> J. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” CoRR, vol. abs/1703.10593, 2017. arXiv: 1703.10593. [Online]. Available: http://arxiv.org/abs/1703.10593.

<a id="Cityscapes_Dataset">[3]</a> M. Cordts, M. Omran, S. Ramos, et al., “The cityscapes dataset for semantic urban scene understanding,” CoRR, vol. abs/1604.01685, 2016. arXiv: 1604.01685. [Online]. Available: http://arxiv.org/abs/1604.01685.

<a id="FCN_score">[4]</a> P. Isola, J. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” CoRR, vol. abs/1611.07004, 2016. arXiv: 1611 . 07004. [Online]. Available: http://arxiv.org/abs/1611.07004.

## Who did what
| Who | What|
| --- | --- |
| Janaína Moreira-Kanaley | Make the model working, training the model on cityscapes, modifitcation of the evaluation script, run evaluation, traing the model with new datasets, work on blogpost, work on the story line |
| Taichi Uno    | Make the model working, modifitcation of the evaluation script, work on blogpost, work on the story line |
